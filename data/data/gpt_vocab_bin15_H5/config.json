{
    "n_ctx": 5024,
    "n_embd": 768,
    "n_head": 12,
    "n_layer": 12,
    "n_positions": 5024,
    "vocab_size": 59849,
    "initializer_range": 0.02,
    "layer_norm_epsilon": 1e-05,
    "gradient_checkpointing": false,
    "use_cache": true,
    "resid_pdrop": 0.1,
    "embd_pdrop": 0.1,
    "attn_pdrop": 0.1,
    "attention_type": "self",
    "max_position_embeddings": 1024
}
